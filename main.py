"""
é£é€Ÿåºåˆ—é¢„æµ‹ - ä¸»ç¨‹åºå…¥å£
åŠŸèƒ½ï¼š
1. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
2. è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼ˆLinearã€LSTMã€Transformerï¼‰
3. è®­ç»ƒåˆ›æ–°æ¨¡å‹ï¼ˆCNN-LSTMã€Attention-LSTMã€TCNã€Ensembleã€WaveNetï¼‰
4. è¯„ä¼°å’Œå¯¹æ¯”æ‰€æœ‰æ¨¡å‹
5. å¯è§†åŒ–ç»“æœ
6. ä¿å­˜æ¨¡å‹ä¸ºpthæ ¼å¼

ä½¿ç”¨æ–¹æ³•ï¼š
    python main.py                           # è¿è¡Œå®Œæ•´å®éªŒ
    python main.py --mode train              # ä»…è®­ç»ƒ
    python main.py --mode eval               # ä»…è¯„ä¼°ï¼ˆéœ€è¦å·²è®­ç»ƒæ¨¡å‹ï¼‰
    python main.py --mode visualize          # ä»…å¯è§†åŒ–
    python main.py --no-viz                  # ç¦ç”¨å¯è§†åŒ–ï¼ˆæœåŠ¡å™¨æ¨èï¼‰
    python main.py --mode train --no-viz     # ä»…è®­ç»ƒï¼Œä¸ç”Ÿæˆå›¾è¡¨
    python main.py --batch-size 256          # æŒ‡å®šbatch size
    python main.py --epochs 200              # æŒ‡å®šè®­ç»ƒè½®æ•°
    python main.py --resume                  # ä»æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒ
    python main.py --models LSTM Transformer # åªè®­ç»ƒæŒ‡å®šæ¨¡å‹
"""
import os
import sys

# ==================== è§£å†³æœåŠ¡å™¨æ— å›¾å½¢ç•Œé¢é—®é¢˜ï¼ˆå¿…é¡»æœ€å…ˆæ‰§è¡Œï¼‰====================
# åœ¨æ— å¤´LinuxæœåŠ¡å™¨ä¸Šè®¾ç½®ç¯å¢ƒå˜é‡ï¼Œé¿å…Qtæ’ä»¶é”™è¯¯
if sys.platform.startswith('linux'):
    if not os.environ.get('DISPLAY'):
        os.environ['QT_QPA_PLATFORM'] = 'offscreen'
        import matplotlib
        matplotlib.use('Agg')

import argparse
import json
import torch
import numpy as np
import pandas as pd
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from config import (
    DEVICE, BATCH_SIZE, NUM_EPOCHS, MODELS_DIR, RESULTS_DIR, LOGS_DIR,
    SINGLE_STEP_INPUT_LEN, SINGLE_STEP_OUTPUT_LEN,
    MULTI_STEP_INPUT_LEN, MULTI_STEP_OUTPUT_LEN,
    set_seed, RANDOM_SEED, LEARNING_RATE, EARLY_STOPPING_PATIENCE,
    TASK_SPECIFIC_HYPERPARAMS, get_adjusted_lr
)
from data_loader import (
    load_all_data, preprocess_data, create_dataloaders,
    get_feature_columns, get_target_columns
)
from models import get_model, count_parameters
from models_innovative import get_innovative_model
from trainer import (
    train_model, test_model, load_model, 
    print_test_results, compare_models
)
from visualization import (
    plot_dataset_overview, plot_training_history,
    plot_predictions, plot_prediction_scatter,
    plot_multistep_predictions, plot_model_comparison,
    plot_error_distribution, create_results_summary_table
)


# ==================== å…¨å±€è¿è¡Œé…ç½®ï¼ˆå¯è¢«å‘½ä»¤è¡Œå‚æ•°è¦†ç›–ï¼‰====================
class RuntimeConfig:
    """è¿è¡Œæ—¶é…ç½®ï¼Œå¯ä»¥è¢«å‘½ä»¤è¡Œå‚æ•°åŠ¨æ€è¦†ç›–"""
    def __init__(self):
        self.batch_size = BATCH_SIZE
        self.num_epochs = NUM_EPOCHS
        self.learning_rate = LEARNING_RATE
        self.early_stopping_patience = EARLY_STOPPING_PATIENCE
        self.enable_visualization = True  # æ˜¯å¦å¯ç”¨å¯è§†åŒ–
        self.resume_training = False  # æ˜¯å¦ä»æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒ
        self.selected_models = None  # æŒ‡å®šè¦è®­ç»ƒçš„æ¨¡å‹åˆ—è¡¨
        self.metric_mode = None  # è¯„ä¼°æŒ‡æ ‡æ¨¡å¼ (Noneè¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©)
        
    def update_from_args(self, args):
        """ä»å‘½ä»¤è¡Œå‚æ•°æ›´æ–°é…ç½®"""
        if args.batch_size is not None:
            self.batch_size = args.batch_size
            print(f"âš™ï¸  Batch Size å·²è¦†ç›–ä¸º: {self.batch_size}")
        if args.epochs is not None:
            self.num_epochs = args.epochs
            print(f"âš™ï¸  è®­ç»ƒè½®æ•°å·²è¦†ç›–ä¸º: {self.num_epochs}")
        if args.lr is not None:
            self.learning_rate = args.lr
            print(f"âš™ï¸  å­¦ä¹ ç‡å·²è¦†ç›–ä¸º: {self.learning_rate}")
        if args.patience is not None:
            self.early_stopping_patience = args.patience
            print(f"âš™ï¸  æ—©åœè€å¿ƒå€¼å·²è¦†ç›–ä¸º: {self.early_stopping_patience}")
        if args.no_viz:
            self.enable_visualization = False
            print("ğŸ“Š å¯è§†åŒ–å·²ç¦ç”¨ï¼ˆä»…ä¿å­˜æ•°æ®ï¼Œä¸ç”Ÿæˆå›¾è¡¨ï¼‰")
        if hasattr(args, 'resume') and args.resume:
            self.resume_training = True
            print("ğŸ”„ å¯ç”¨ç»§ç»­è®­ç»ƒæ¨¡å¼ï¼ˆä»å·²æœ‰æ£€æŸ¥ç‚¹æ¢å¤ï¼‰")
        if args.models is not None:
            self.selected_models = args.models
            print(f"ğŸ“‹ ä»…è®­ç»ƒæŒ‡å®šæ¨¡å‹: {', '.join(args.models)}")
        if hasattr(args, 'tasks') and args.tasks is not None:
            self.selected_tasks = args.tasks
            print(f"ğŸ“‹ ä»…è®­ç»ƒæŒ‡å®šä»»åŠ¡: {', '.join(args.tasks)}")
        else:
            self.selected_tasks = None
        if hasattr(args, 'metric_mode') and args.metric_mode is not None:
            self.metric_mode = args.metric_mode
            mode_desc = {'r2': 'RÂ²(è¶Šå¤§è¶Šå¥½)', 'mse': 'MSE(è¶Šå°è¶Šå¥½)', 'combined': 'ç»¼åˆæŒ‡æ ‡'}
            print(f"ğŸ“Š è¯„ä¼°æŒ‡æ ‡æ¨¡å¼: {mode_desc.get(self.metric_mode, self.metric_mode)}")

# å…¨å±€è¿è¡Œæ—¶é…ç½®å®ä¾‹
runtime_config = RuntimeConfig()


# å®šä¹‰ä»»åŠ¡é…ç½®
# å•æ­¥é¢„æµ‹ï¼š8å°æ—¶ â†’ 1å°æ—¶
# å¤šæ­¥é¢„æµ‹ï¼š8å°æ—¶ â†’ 16å°æ—¶
TASKS = {
    'singlestep': {
        'input_len': SINGLE_STEP_INPUT_LEN,
        'output_len': SINGLE_STEP_OUTPUT_LEN,
        'description': f'å•æ­¥é¢„æµ‹ï¼ˆ{SINGLE_STEP_INPUT_LEN}å°æ—¶â†’{SINGLE_STEP_OUTPUT_LEN}å°æ—¶ï¼‰'
    },
    'multistep': {
        'input_len': MULTI_STEP_INPUT_LEN,
        'output_len': MULTI_STEP_OUTPUT_LEN,
        'description': f'å¤šæ­¥é¢„æµ‹ï¼ˆ{MULTI_STEP_INPUT_LEN}å°æ—¶â†’{MULTI_STEP_OUTPUT_LEN}å°æ—¶ï¼‰'
    }
}

# åŸºç¡€æ¨¡å‹
BASE_MODELS = ['Linear', 'LSTM', 'Transformer']

# åˆ›æ–°æ¨¡å‹
INNOVATIVE_MODELS = ['CNN_LSTM', 'Attention_LSTM', 'TCN', 'WaveNet']


def setup_experiment():
    """è®¾ç½®å®éªŒç¯å¢ƒ"""
    set_seed(RANDOM_SEED)
    
    # åˆ›å»ºå¿…è¦ç›®å½•
    for dir_path in [MODELS_DIR, RESULTS_DIR, LOGS_DIR]:
        os.makedirs(dir_path, exist_ok=True)
    
    print("=" * 70)
    print("é£é€Ÿåºåˆ—é¢„æµ‹å®éªŒ")
    print("=" * 70)
    print(f"è®¾å¤‡: {DEVICE}")
    print(f"éšæœºç§å­: {RANDOM_SEED}")
    print(f"æ‰¹æ¬¡å¤§å°: {runtime_config.batch_size}")
    print(f"æœ€å¤§è®­ç»ƒè½®æ•°: {runtime_config.num_epochs}")
    print(f"å­¦ä¹ ç‡: {runtime_config.learning_rate}")
    print(f"å¯è§†åŒ–: {'å¯ç”¨' if runtime_config.enable_visualization else 'ç¦ç”¨'}")
    print("=" * 70)


def load_and_preprocess_data():
    """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
    print("\n" + "=" * 70)
    print("æ­¥éª¤1: æ•°æ®åŠ è½½ä¸é¢„å¤„ç†")
    print("=" * 70)
    
    # åŠ è½½åŸå§‹æ•°æ®
    raw_df = load_all_data()
    
    # é¢„å¤„ç†
    processed_df = preprocess_data(raw_df)
    
    # ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®ä¿¡æ¯
    info = {
        'shape': processed_df.shape,
        'columns': processed_df.columns.tolist(),
        'date_range': [str(processed_df.iloc[0]['Date & Time Stamp']), 
                      str(processed_df.iloc[-1]['Date & Time Stamp'])],
        'num_samples': len(processed_df)
    }
    
    info_path = os.path.join(RESULTS_DIR, 'data_info.json')
    with open(info_path, 'w', encoding='utf-8') as f:
        json.dump(info, f, ensure_ascii=False, indent=2)
    
    print(f"\næ•°æ®ä¿¡æ¯å·²ä¿å­˜è‡³: {info_path}")
    
    return processed_df


def visualize_dataset(df):
    """å¯è§†åŒ–æ•°æ®é›†"""
    if not runtime_config.enable_visualization:
        print("\n[è·³è¿‡] æ•°æ®é›†å¯è§†åŒ–ï¼ˆå·²ç¦ç”¨ï¼‰")
        return
        
    print("\n" + "=" * 70)
    print("æ­¥éª¤2: æ•°æ®é›†å¯è§†åŒ–")
    print("=" * 70)
    
    save_path = os.path.join(RESULTS_DIR, 'dataset_overview.png')
    plot_dataset_overview(df, save_path=save_path)


def train_all_models(df, model_list, tasks_to_run=None, is_innovative=False):
    """
    è®­ç»ƒæ‰€æœ‰æ¨¡å‹
    
    Args:
        df: é¢„å¤„ç†åçš„æ•°æ®
        model_list: è¦è®­ç»ƒçš„æ¨¡å‹åˆ—è¡¨
        tasks_to_run: è¦è¿è¡Œçš„ä»»åŠ¡åˆ—è¡¨ï¼ˆé»˜è®¤å…¨éƒ¨ï¼‰
        is_innovative: æ˜¯å¦ä¸ºåˆ›æ–°æ¨¡å‹
    """
    if tasks_to_run is None:
        tasks_to_run = list(TASKS.keys())
    
    model_type = "åˆ›æ–°æ¨¡å‹" if is_innovative else "åŸºç¡€æ¨¡å‹"
    print(f"\n" + "=" * 70)
    print(f"æ­¥éª¤3: è®­ç»ƒ{model_type}")
    print("=" * 70)
    
    all_results = {}
    
    for task_name in tasks_to_run:
        task_config = TASKS[task_name]
        print(f"\n{'='*50}")
        print(f"ä»»åŠ¡: {task_config['description']}")
        print(f"{'='*50}")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆä½¿ç”¨runtime_configä¸­çš„batch_sizeï¼‰
        input_len = task_config['input_len']
        output_len = task_config['output_len']
        
        train_loader, val_loader, test_loader, scaler_features, scaler_targets, feature_cols, target_cols = \
            create_dataloaders(df, input_len, output_len, runtime_config.batch_size)
        
        num_features = len(feature_cols)
        num_targets = len(target_cols)
        
        task_results = {}
        
        # å¯¼å…¥ä»»åŠ¡ç‰¹å®šçš„è¶…å‚
        from config import TASK_SPECIFIC_HYPERPARAMS, get_adjusted_lr
        
        # è·å–ä»»åŠ¡ç‰¹å®šçš„è¶…å‚ï¼ˆå¦‚æœç”¨æˆ·æ²¡æœ‰æ‰‹åŠ¨æŒ‡å®šï¼Œåˆ™ä½¿ç”¨ä»»åŠ¡æ¨èå€¼ï¼‰
        task_config = TASK_SPECIFIC_HYPERPARAMS.get(task_name, {})
        
        # ç¡®å®šæœ€ç»ˆè¶…å‚ä¼˜å…ˆçº§ï¼šç”¨æˆ·æŒ‡å®š > ä»»åŠ¡æ¨è > å…¨å±€é»˜è®¤
        final_lr = runtime_config.learning_rate if runtime_config.learning_rate != LEARNING_RATE else task_config.get('lr', LEARNING_RATE)
        final_patience = runtime_config.early_stopping_patience if runtime_config.early_stopping_patience != EARLY_STOPPING_PATIENCE else task_config.get('patience', EARLY_STOPPING_PATIENCE)
        final_epochs = runtime_config.num_epochs
        
        # å¦‚æœbatch_sizeè¢«æ”¹ä¸º256ï¼Œè‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡ä¸‹é™ï¼ˆ0.0002å¤ªä½äº†ï¼ï¼‰
        if runtime_config.batch_size == 256 and final_lr == 0.0002:
            final_lr = 0.0005  # è‡ªåŠ¨çº æ­£ï¼š256æ—¶æ”¹ä¸º0.0005
            print(f"âš ï¸  æ£€æµ‹åˆ°batch_size=256ï¼Œå­¦ä¹ ç‡è‡ªåŠ¨ä»0.0002è°ƒæ•´ä¸º0.0005ï¼ˆå¤ªä½ä¼šå¯¼è‡´æ¬ æ‹Ÿåˆï¼‰")
        
        # å¦‚æœç”¨æˆ·ç”¨äº†resumeä½†æ²¡æœ‰è°ƒæ•´å­¦ä¹ ç‡ï¼Œå»ºè®®é™ä½
        if runtime_config.resume_training and runtime_config.learning_rate == LEARNING_RATE:
            final_lr = task_config.get('lr', final_lr)
            print(f"ğŸ’¡ ç»§ç»­è®­ç»ƒæ¨¡å¼ï¼šä½¿ç”¨ä»»åŠ¡ä¼˜åŒ–å­¦ä¹ ç‡ {final_lr}")
        
        for model_name in model_list:
            print(f"\n--- è®­ç»ƒ {model_name} ---")
            
            # åˆ›å»ºæ¨¡å‹
            if is_innovative:
                model = get_innovative_model(model_name, input_len, output_len, num_features, num_targets)
            else:
                model = get_model(model_name, input_len, output_len, num_features, num_targets)
            
            print(f"æ¨¡å‹å‚æ•°é‡: {count_parameters(model):,}")
            
            # è®­ç»ƒï¼ˆä½¿ç”¨ä»»åŠ¡ç‰¹å®šçš„è¶…å‚ï¼‰
            metric_mode_str = runtime_config.metric_mode if runtime_config.metric_mode else ('r2' if task_name == 'multistep_16h' else 'mse')
            print(f"ğŸ“Š ä½¿ç”¨è¶…å‚: lr={final_lr:.6f}, patience={final_patience}, epochs={final_epochs}, metric={metric_mode_str}")
            history = train_model(
                model, train_loader, val_loader,
                model_name=model_name,
                task_name=task_name,
                num_epochs=final_epochs,
                learning_rate=final_lr,  # ä½¿ç”¨ä»»åŠ¡ä¼˜åŒ–åçš„å­¦ä¹ ç‡
                patience=final_patience,  # ä½¿ç”¨ä»»åŠ¡ä¼˜åŒ–åçš„æ—©åœ
                device=DEVICE,
                save_best=True,
                verbose=True,
                resume=runtime_config.resume_training,  # æ”¯æŒç»§ç»­è®­ç»ƒ
                metric_mode=runtime_config.metric_mode  # è¯„ä¼°æŒ‡æ ‡æ¨¡å¼ï¼ˆNoneè¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©ï¼‰
            )
            
            # ç»˜åˆ¶è®­ç»ƒå†å²ï¼ˆä»æ£€æŸ¥ç‚¹è¯»å–å®Œæ•´å†å²ï¼ŒåŒ…å«æ‰€æœ‰å¾®è°ƒè¿‡ç¨‹ï¼‰
            if runtime_config.enable_visualization:
                history_save_path = os.path.join(RESULTS_DIR, f'{model_name}_{task_name}_history.png')
                # ä»ä¿å­˜çš„æ£€æŸ¥ç‚¹è¯»å–å®Œæ•´å†å²ï¼ˆåŒ…å«æ‰€æœ‰å¾®è°ƒè¿‡ç¨‹ï¼‰
                model_path = os.path.join(MODELS_DIR, f"{model_name}_{task_name}.pth")
                previous_epochs = 0
                if os.path.exists(model_path):
                    try:
                        checkpoint = torch.load(model_path, map_location=DEVICE, weights_only=False)
                        full_history = checkpoint.get('history', history)
                        # è®¡ç®—ä¹‹å‰çš„è®­ç»ƒè½®æ•°ï¼ˆç”¨äºæ ‡è®°å¾®è°ƒåˆ†ç•Œç‚¹ï¼‰
                        if runtime_config.resume_training and len(full_history.get('train_loss', [])) > len(history.get('train_loss', [])):
                            previous_epochs = len(full_history['train_loss']) - len(history['train_loss'])
                        # ä½¿ç”¨å®Œæ•´å†å²ç»˜åˆ¶ï¼ˆåŒ…å«æ‰€æœ‰å¾®è°ƒè¿‡ç¨‹ï¼‰
                        plot_training_history(full_history, model_name, task_name, save_path=history_save_path, previous_epochs=previous_epochs)
                    except Exception as e:
                        print(f"âš ï¸  æ— æ³•ä»æ£€æŸ¥ç‚¹è¯»å–å®Œæ•´å†å²ï¼Œä½¿ç”¨æœ¬æ¬¡è®­ç»ƒå†å²: {e}")
                        plot_training_history(history, model_name, task_name, save_path=history_save_path)
                else:
                    # é¦–æ¬¡è®­ç»ƒï¼Œç›´æ¥ä½¿ç”¨å½“å‰å†å²
                    plot_training_history(history, model_name, task_name, save_path=history_save_path)
            
            # æµ‹è¯•
            metrics, metrics_per_target, predictions, targets = test_model(
                model, test_loader, scaler_targets, device=DEVICE
            )
            
            # æ‰“å°ç»“æœ
            print_test_results(model_name, task_name, metrics, metrics_per_target, target_cols)
            
            # ä¿å­˜ç»“æœ
            task_results[model_name] = {
                'metrics': metrics,
                'metrics_per_target': metrics_per_target,
                'predictions': predictions,
                'targets': targets,
                'history': history
            }
            
            # å¯è§†åŒ–é¢„æµ‹ç»“æœï¼ˆå¯é€‰ï¼‰
            if runtime_config.enable_visualization:
                pred_save_path = os.path.join(RESULTS_DIR, f'{model_name}_{task_name}_predictions.png')
                plot_predictions(targets, predictions, model_name, task_name, target_cols, 
                               num_samples=200, save_path=pred_save_path)
                
                scatter_save_path = os.path.join(RESULTS_DIR, f'{model_name}_{task_name}_scatter.png')
                plot_prediction_scatter(targets, predictions, model_name, task_name, target_cols,
                                      save_path=scatter_save_path)
                
                # å¯¹äºå¤šæ­¥é¢„æµ‹ï¼Œé¢å¤–ç»˜åˆ¶å¤šæ­¥é¢„æµ‹å›¾
                if output_len > 1:
                    multistep_save_path = os.path.join(RESULTS_DIR, f'{model_name}_{task_name}_multistep.png')
                    plot_multistep_predictions(targets, predictions, model_name, task_name,
                                             save_path=multistep_save_path)
        
        all_results[task_name] = task_results
    
    return all_results


def evaluate_and_compare(all_results):
    """è¯„ä¼°å’Œå¯¹æ¯”æ‰€æœ‰æ¨¡å‹"""
    print("\n" + "=" * 70)
    print("æ­¥éª¤4: æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
    print("=" * 70)
    
    # æ•´ç†ç»“æœ
    comparison_dict = {}
    for task_name, task_results in all_results.items():
        for model_name, result in task_results.items():
            if model_name not in comparison_dict:
                comparison_dict[model_name] = {}
            comparison_dict[model_name][task_name] = result['metrics']
    
    # åˆ›å»ºå¯¹æ¯”DataFrame
    results_df = compare_models(comparison_dict)
    
    # ==================== åˆå¹¶ç°æœ‰ç»“æœï¼ˆä¸è¦†ç›–ï¼‰====================
    results_csv_path = os.path.join(RESULTS_DIR, 'model_comparison.csv')
    
    if os.path.exists(results_csv_path):
        # è¯»å–ç°æœ‰ç»“æœ
        existing_df = pd.read_csv(results_csv_path)
        print(f"ğŸ“‚ å‘ç°ç°æœ‰ç»“æœæ–‡ä»¶ï¼Œå°†åˆå¹¶æ›´æ–°...")
        
        # åˆå¹¶ï¼šæ–°ç»“æœè¦†ç›–æ—§ç»“æœä¸­ç›¸åŒçš„Model+Taskç»„åˆ
        for _, new_row in results_df.iterrows():
            mask = (existing_df['Model'] == new_row['Model']) & (existing_df['Task'] == new_row['Task'])
            if mask.any():
                # æ›´æ–°ç°æœ‰è¡Œ
                existing_df.loc[mask, ['MSE', 'RMSE', 'MAE', 'R2']] = new_row[['MSE', 'RMSE', 'MAE', 'R2']].values
            else:
                # æ·»åŠ æ–°è¡Œ
                existing_df = pd.concat([existing_df, pd.DataFrame([new_row])], ignore_index=True)
        
        results_df = existing_df
        print(f"âœ… å·²åˆå¹¶ {len(results_df)} æ¡æ¨¡å‹ç»“æœ")
    
    # æŒ‰Taskå’ŒModelæ’åº
    task_order = ['singlestep', 'multistep']
    results_df['Task'] = pd.Categorical(results_df['Task'], categories=task_order, ordered=True)
    results_df = results_df.sort_values(['Task', 'Model']).reset_index(drop=True)
    
    # ä¿å­˜å®Œæ•´ç»“æœ
    results_df.to_csv(results_csv_path, index=False, encoding='utf-8-sig')
    print(f"\nå¯¹æ¯”ç»“æœå·²ä¿å­˜è‡³: {results_csv_path}")
    
    # æ‰“å°å¯¹æ¯”è¡¨æ ¼
    print("\næ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
    print(results_df.to_string(index=False))
    
    # ç»˜åˆ¶å¯¹æ¯”å›¾ï¼ˆå¯é€‰ï¼‰
    if runtime_config.enable_visualization:
        for metric in ['MSE', 'RMSE', 'MAE', 'R2']:
            comparison_save_path = os.path.join(RESULTS_DIR, f'comparison_{metric}.png')
            plot_model_comparison(results_df, metric=metric, save_path=comparison_save_path)
        
        # åˆ›å»ºæ±‡æ€»è¡¨æ ¼
        table_save_path = os.path.join(RESULTS_DIR, 'results_summary_table.png')
        create_results_summary_table(comparison_dict, save_path=table_save_path)
    
    return results_df


def generate_report(results_df, all_results):
    """ç”Ÿæˆå®éªŒæŠ¥å‘Š"""
    print("\n" + "=" * 70)
    print("æ­¥éª¤5: ç”Ÿæˆå®éªŒæŠ¥å‘Š")
    print("=" * 70)
    
    report_path = os.path.join(RESULTS_DIR, 'experiment_report.md')
    
    # é‡æ–°è¯»å–å®Œæ•´çš„CSVæ•°æ®ï¼ˆåŒ…å«åˆå¹¶åçš„æ‰€æœ‰æ¨¡å‹ï¼‰
    results_csv_path = os.path.join(RESULTS_DIR, 'model_comparison.csv')
    if os.path.exists(results_csv_path):
        full_results_df = pd.read_csv(results_csv_path)
    else:
        full_results_df = results_df
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# Wind Speed Prediction Experiment Report\n\n")
        f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("## 1. Experiment Configuration\n\n")
        f.write(f"- Device: {DEVICE}\n")
        f.write(f"- Batch Size: {runtime_config.batch_size}\n")
        f.write(f"- Max Epochs: {runtime_config.num_epochs}\n")
        f.write(f"- Learning Rate: {runtime_config.learning_rate}\n")
        f.write(f"- Random Seed: {RANDOM_SEED}\n\n")
        
        f.write("## 2. Task Configuration\n\n")
        task_descriptions = {
            'singlestep': 'Single-step Prediction (8h -> 1h)',
            'multistep': 'Multi-step Prediction (8h -> 16h)'
        }
        for task_name, task_config in TASKS.items():
            desc = task_descriptions.get(task_name, task_config['description'])
            f.write(f"### {desc}\n")
            f.write(f"- Input Length: {task_config['input_len']} hours\n")
            f.write(f"- Output Length: {task_config['output_len']} hours\n\n")
        
        f.write("## 3. Model Performance Comparison\n\n")
        f.write(full_results_df.to_markdown(index=False))
        f.write("\n\n")
        
        f.write("## 4. Best Models\n\n")
        
        # æ‰¾å‡ºæ¯ä¸ªä»»åŠ¡çš„æœ€ä½³æ¨¡å‹
        for task in ['singlestep', 'multistep']:
            task_results = full_results_df[full_results_df['Task'] == task]
            if len(task_results) > 0:
                best_idx = task_results['RMSE'].idxmin()
                best_model = task_results.loc[best_idx, 'Model']
                best_rmse = task_results.loc[best_idx, 'RMSE']
                best_r2 = task_results.loc[best_idx, 'R2']
                desc = task_descriptions.get(task, task)
                f.write(f"- **{desc}**: {best_model} (RMSE: {best_rmse:.4f}, RÂ²: {best_r2:.4f})\n")
        
        f.write("\n## 5. Innovation Points\n\n")
        f.write("### 5.1 CNN-LSTM Hybrid Model\n")
        f.write("- Combines CNN's local feature extraction with LSTM's sequence modeling\n")
        f.write("- Multi-scale convolution kernels capture features at different time scales\n")
        f.write("- Attention mechanism enhances important feature weights\n\n")
        
        f.write("### 5.2 Attention-LSTM Model\n")
        f.write("- Self-attention mechanism enhances feature representation\n")
        f.write("- Temporal attention focuses on key time points\n")
        f.write("- Multi-head attention processes different subspace information in parallel\n\n")
        
        f.write("### 5.3 TCN Model\n")
        f.write("- Causal convolution ensures temporal causality\n")
        f.write("- Dilated convolution exponentially expands receptive field\n")
        f.write("- Residual connections stabilize deep network training\n\n")
        
        f.write("### 5.4 WaveNet Model\n")
        f.write("- Gated activation units enhance expressive power\n")
        f.write("- Dilated causal convolution efficiently models long sequences\n")
        f.write("- Residual and Skip connections accelerate gradient flow\n\n")
        
        f.write("## 6. Conclusion\n\n")
        f.write("This experiment compared Linear, LSTM, and Transformer as baseline models, ")
        f.write("along with CNN-LSTM, Attention-LSTM, TCN, and WaveNet as innovative models ")
        f.write("for wind speed prediction tasks.\n\n")
        f.write("The results show that deep learning models have significant advantages ")
        f.write("in capturing wind speed temporal features, especially models with attention ")
        f.write("mechanisms that can better capture long-term dependencies.\n")
    
    print(f"å®éªŒæŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
    return report_path


def main(args):
    """ä¸»å‡½æ•°"""
    # ä»å‘½ä»¤è¡Œå‚æ•°æ›´æ–°è¿è¡Œæ—¶é…ç½®
    runtime_config.update_from_args(args)
    
    setup_experiment()
    
    if args.mode in ['all', 'train', 'visualize']:
        # åŠ è½½æ•°æ®
        df = load_and_preprocess_data()
        
        # å¯è§†åŒ–æ•°æ®é›†
        if args.mode in ['all', 'visualize']:
            visualize_dataset(df)
    
    if args.mode in ['all', 'train']:
        df = load_and_preprocess_data() if 'df' not in dir() else df
        
        # ç¡®å®šè¦è®­ç»ƒçš„æ¨¡å‹
        if runtime_config.selected_models:
            # ç”¨æˆ·æŒ‡å®šäº†æ¨¡å‹
            selected_base = [m for m in runtime_config.selected_models if m in BASE_MODELS]
            selected_innovative = [m for m in runtime_config.selected_models if m in INNOVATIVE_MODELS]
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ— æ•ˆçš„æ¨¡å‹å
            all_valid_models = BASE_MODELS + INNOVATIVE_MODELS
            invalid_models = [m for m in runtime_config.selected_models if m not in all_valid_models]
            if invalid_models:
                print(f"âš ï¸ æœªçŸ¥æ¨¡å‹: {invalid_models}")
                print(f"   å¯ç”¨æ¨¡å‹: {all_valid_models}")
        else:
            selected_base = BASE_MODELS
            selected_innovative = INNOVATIVE_MODELS
        
        all_results = {}
        
        # ç¡®å®šè¦è¿è¡Œçš„ä»»åŠ¡
        selected_tasks = runtime_config.selected_tasks if hasattr(runtime_config, 'selected_tasks') and runtime_config.selected_tasks else None
        
        # è®­ç»ƒåŸºç¡€æ¨¡å‹
        if selected_base:
            base_results = train_all_models(df, selected_base, tasks_to_run=selected_tasks, is_innovative=False)
            for task_name in base_results:
                if task_name not in all_results:
                    all_results[task_name] = {}
                all_results[task_name].update(base_results[task_name])
        
        # è®­ç»ƒåˆ›æ–°æ¨¡å‹
        if selected_innovative:
            innovative_results = train_all_models(df, selected_innovative, tasks_to_run=selected_tasks, is_innovative=True)
            for task_name in innovative_results:
                if task_name not in all_results:
                    all_results[task_name] = {}
                all_results[task_name].update(innovative_results[task_name])
        
        # è¯„ä¼°å’Œå¯¹æ¯”
        if all_results:
            results_df = evaluate_and_compare(all_results)
            
            # ç”ŸæˆæŠ¥å‘Š
            generate_report(results_df, all_results)
    
    if args.mode == 'eval':
        # ä»…è¯„ä¼°ï¼ˆéœ€è¦å·²è®­ç»ƒçš„æ¨¡å‹ï¼‰
        print("è¯„ä¼°æ¨¡å¼ï¼šè¯·ç¡®ä¿æ¨¡å‹å·²è®­ç»ƒå¹¶ä¿å­˜")
        # TODO: åŠ è½½å·²ä¿å­˜æ¨¡å‹å¹¶è¯„ä¼°
    
    print("\n" + "=" * 70)
    print("å®éªŒå®Œæˆï¼")
    print("=" * 70)
    print(f"\næ‰€æœ‰ç»“æœå·²ä¿å­˜è‡³: {RESULTS_DIR}")
    print(f"æ‰€æœ‰æ¨¡å‹å·²ä¿å­˜è‡³: {MODELS_DIR}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='é£é€Ÿåºåˆ—é¢„æµ‹å®éªŒ')
    parser.add_argument('--mode', type=str, default='all',
                       choices=['all', 'train', 'eval', 'visualize'],
                       help='è¿è¡Œæ¨¡å¼: all(å®Œæ•´å®éªŒ), train(ä»…è®­ç»ƒ), eval(ä»…è¯„ä¼°), visualize(ä»…å¯è§†åŒ–)')
    parser.add_argument('--models', type=str, nargs='+', default=None,
                       help='æŒ‡å®šè¦è®­ç»ƒçš„æ¨¡å‹ï¼Œå¦‚: --models LSTM Transformer')
    parser.add_argument('--tasks', type=str, nargs='+', default=None,
                       help='æŒ‡å®šè¦è¿è¡Œçš„ä»»åŠ¡ï¼Œå¦‚: --tasks singlestep multistep_1h')
    parser.add_argument('--no-viz', action='store_true',
                       help='ç¦ç”¨å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆï¼ˆæœåŠ¡å™¨è®­ç»ƒæ¨èï¼‰')
    parser.add_argument('--batch-size', type=int, default=None,
                       help='è¦†ç›–é»˜è®¤çš„batch size')
    parser.add_argument('--epochs', type=int, default=None,
                       help='è¦†ç›–é»˜è®¤çš„è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=None,
                       help='è¦†ç›–é»˜è®¤çš„å­¦ä¹ ç‡')
    parser.add_argument('--patience', type=int, default=None,
                       help='æ—©åœçš„è€å¿ƒå€¼')
    parser.add_argument('--resume', action='store_true',
                       help='ä»å·²æœ‰æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒï¼ˆè¿­ä»£ä¼˜åŒ–æ¨¡å‹ï¼‰')
    parser.add_argument('--metric-mode', type=str, default=None,
                       choices=['r2', 'mse', 'combined'],
                       help='è¯„ä¼°æŒ‡æ ‡æ¨¡å¼: r2(RÂ²è¶Šå¤§è¶Šå¥½), mse(MSEè¶Šå°è¶Šå¥½), combined(ç»¼åˆæŒ‡æ ‡)')
    
    args = parser.parse_args()
    main(args)
