#!/usr/bin/env python3
"""
æ¨¡å‹æœ€ä½³ç‰ˆæœ¬æ¢å¤å·¥å…·

é—®é¢˜ï¼šç”±äºä¹‹å‰çš„bugï¼ŒæŸäº›æ¨¡å‹çš„æœ€ä½³ç‰ˆæœ¬å¯èƒ½è¢«æ›´å·®çš„ç‰ˆæœ¬è¦†ç›–
è§£å†³æ–¹æ¡ˆï¼šä»å†å²è®°å½•ä¸­æ¢å¤æœ€ä½³æ¨¡å‹

ç”¨æ³•ï¼š
    python recover_best_models.py --input results/model_comparison.csv
"""

import os
import json
import pandas as pd
import torch
import argparse
from pathlib import Path

def analyze_model_history():
    """
    åˆ†ææ‰€æœ‰è®­ç»ƒæ¨¡å‹çš„å†å²è®°å½•
    æ‰¾å‡ºå“ªäº›æ¨¡å‹å½“å‰ç‰ˆæœ¬ä¸å¦‚å†å²æœ€ä½³
    """
    results_dir = Path('results')
    models_dir = Path('models')
    
    print("\n" + "="*70)
    print("æ¨¡å‹å†å²ç‰ˆæœ¬åˆ†æ")
    print("="*70)
    
    # è¯»å–CSV
    csv_file = results_dir / 'model_comparison.csv'
    if not csv_file.exists():
        print(f"âŒ æ‰¾ä¸åˆ° {csv_file}")
        return
    
    df = pd.read_csv(csv_file)
    print(f"\nğŸ“Š å½“å‰æ¨¡å‹æ•°æ®åº“: {len(df)} æ¡è®°å½•")
    print(df.head(10))
    
    # éå†æ‰€æœ‰æ¨¡å‹æ£€æŸ¥ç‚¹
    print("\n" + "-"*70)
    print("æ£€æŸ¥ç‚¹å†å²åˆ†æï¼š")
    print("-"*70)
    
    issues = []
    
    for pth_file in sorted(models_dir.glob("*.pth")):
        try:
            checkpoint = torch.load(pth_file, map_location='cpu', weights_only=False)
            model_name = checkpoint.get('model_name', 'unknown')
            task_name = checkpoint.get('task_name', 'unknown')
            history = checkpoint.get('history', {})
            
            if not history:
                continue
            
            best_val_loss = history.get('best_val_loss', float('inf'))
            best_epoch = history.get('best_epoch', 0)
            total_epochs = len(history.get('train_loss', []))
            
            # ä»CSVä¸­æŸ¥æ‰¾å½“å‰è®°å½•
            csv_record = df[(df['Model'] == model_name) & (df['Task'] == task_name)]
            
            if csv_record.empty:
                print(f"\nâš ï¸  {model_name}_{task_name}: æ£€æŸ¥ç‚¹å­˜åœ¨ä½†ä¸åœ¨CSVä¸­")
                issues.append({
                    'model': model_name,
                    'task': task_name,
                    'issue': 'CSVä¸­ç¼ºå¤±',
                    'checkpoint_loss': best_val_loss,
                    'file': pth_file.name
                })
            else:
                csv_mse = float(csv_record['MSE'].values[0])
                checkpoint_loss = best_val_loss
                
                # MSEå’ŒéªŒè¯æŸå¤±åº”è¯¥å¤§è‡´æˆæ­£æ¯”
                if abs(csv_mse - checkpoint_loss) > 0.1:
                    print(f"\nâš ï¸  {model_name}_{task_name}: ç‰ˆæœ¬ä¸ä¸€è‡´")
                    print(f"   æ£€æŸ¥ç‚¹æŸå¤±: {checkpoint_loss:.4f}")
                    print(f"   CSVä¸­MSE:  {csv_mse:.4f}")
                    issues.append({
                        'model': model_name,
                        'task': task_name,
                        'issue': 'CSVä¸æ£€æŸ¥ç‚¹ä¸ä¸€è‡´',
                        'checkpoint_loss': checkpoint_loss,
                        'csv_mse': csv_mse,
                        'file': pth_file.name
                    })
                else:
                    print(f"âœ… {model_name}_{task_name}: ä¸€è‡´ (loss={checkpoint_loss:.4f}, epochs={total_epochs})")
        
        except Exception as e:
            print(f"âŒ è¯»å– {pth_file.name} å¤±è´¥: {e}")
    
    # æ€»ç»“
    print("\n" + "="*70)
    if issues:
        print(f"âš ï¸  å‘ç° {len(issues)} ä¸ªé—®é¢˜ï¼š\n")
        for issue in issues:
            print(f"  - {issue['model']}_{issue['task']}: {issue['issue']}")
            print(f"    æ–‡ä»¶: {issue['file']}")
    else:
        print("âœ… æ‰€æœ‰æ¨¡å‹ç‰ˆæœ¬ä¸€è‡´ï¼Œæ²¡æœ‰å‘ç°é—®é¢˜")
    print("="*70 + "\n")

def compare_with_csv():
    """
    å¯¹æ¯”CSVä¸­å„ä»»åŠ¡çš„æœ€ä½³æ¨¡å‹
    """
    csv_file = Path('results/model_comparison.csv')
    
    if not csv_file.exists():
        print(f"âŒ æ‰¾ä¸åˆ° {csv_file}")
        return
    
    df = pd.read_csv(csv_file)
    
    print("\n" + "="*70)
    print("å„ä»»åŠ¡æœ€ä½³æ¨¡å‹æ’å")
    print("="*70)
    
    for task in df['Task'].unique():
        task_df = df[df['Task'] == task].sort_values('R2', ascending=False)
        print(f"\nğŸ“ {task}:")
        print("-" * 70)
        
        for idx, row in task_df.iterrows():
            print(f"  {row['Model']:20} RÂ²={row['R2']:.4f}  RMSE={row['RMSE']:.4f}")
        
        # æ˜¾ç¤ºtop 3
        print(f"\n  ğŸ¥‡ æœ€ä½³: {task_df.iloc[0]['Model']} (RÂ²={task_df.iloc[0]['R2']:.4f})")
        if len(task_df) > 1:
            print(f"  ğŸ¥ˆ æ¬¡ä¼˜: {task_df.iloc[1]['Model']} (RÂ²={task_df.iloc[1]['R2']:.4f})")
        if len(task_df) > 2:
            print(f"  ğŸ¥‰ ç¬¬ä¸‰: {task_df.iloc[2]['Model']} (RÂ²={task_df.iloc[2]['R2']:.4f})")
    
    print("\n" + "="*70 + "\n")

def show_improvement_opportunities():
    """
    æ˜¾ç¤ºå“ªäº›æ¨¡å‹æœ‰æ”¹è¿›ç©ºé—´
    """
    csv_file = Path('results/model_comparison.csv')
    
    if not csv_file.exists():
        return
    
    df = pd.read_csv(csv_file)
    
    print("\n" + "="*70)
    print("æ”¹è¿›æœºä¼šåˆ†æï¼ˆå“ªäº›æ¨¡å‹å¯èƒ½è¢«æ¬¡ä¼˜ç‰ˆæœ¬è¦†ç›–ï¼‰")
    print("="*70)
    
    for task in df['Task'].unique():
        task_df = df[df['Task'] == task]
        
        # æ‰¾å‡ºRÂ²æœ€ä½çš„æ¨¡å‹ï¼ˆå¯èƒ½æ˜¯è¢«åç‰ˆæœ¬è¦†ç›–äº†ï¼‰
        worst_models = task_df.nsmallest(3, 'R2')
        
        print(f"\nğŸ“ {task} - å¯èƒ½éœ€è¦æ¢å¤çš„æ¨¡å‹ï¼š")
        for idx, row in worst_models.iterrows():
            print(f"  - {row['Model']:20} RÂ²={row['R2']:.4f} (MSE={row['MSE']:.4f})")
            print(f"    å»ºè®®ï¼šé‡æ–°è®­ç»ƒæˆ–æ£€æŸ¥æ˜¯å¦è¢«åç‰ˆæœ¬è¦†ç›–")
    
    print("\n" + "="*70 + "\n")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="æ¨¡å‹æœ€ä½³ç‰ˆæœ¬æ¢å¤å·¥å…·")
    parser.add_argument('--analyze', action='store_true', help='åˆ†ææ‰€æœ‰æ¨¡å‹å†å²')
    parser.add_argument('--compare', action='store_true', help='å¯¹æ¯”CSVä¸­çš„æ¨¡å‹')
    parser.add_argument('--opportunities', action='store_true', help='æ˜¾ç¤ºæ”¹è¿›æœºä¼š')
    parser.add_argument('--all', action='store_true', help='è¿è¡Œæ‰€æœ‰åˆ†æ')
    
    args = parser.parse_args()
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šä»»ä½•å‚æ•°ï¼Œè¿è¡Œæ‰€æœ‰åˆ†æ
    if not any([args.analyze, args.compare, args.opportunities, args.all]):
        args.all = True
    
    if args.all or args.analyze:
        analyze_model_history()
    
    if args.all or args.compare:
        compare_with_csv()
    
    if args.all or args.opportunities:
        show_improvement_opportunities()
