"""
é…ç½®æ–‡ä»¶ï¼šå®šä¹‰æ‰€æœ‰è¶…å‚æ•°å’Œè·¯å¾„é…ç½®
æ”¯æŒè‡ªåŠ¨æ£€æµ‹GPUï¼Œé€‚é…æœ¬åœ°Windowså’Œè¿œç¨‹LinuxæœåŠ¡å™¨è®­ç»ƒ
"""
import os
import sys
# ==================== è§£å†³æœåŠ¡å™¨æ— å›¾å½¢ç•Œé¢é—®é¢˜ï¼ˆå¿…é¡»æœ€å…ˆæ‰§è¡Œï¼‰====================
# åœ¨æ— å¤´LinuxæœåŠ¡å™¨ä¸Šè®¾ç½®ç¯å¢ƒå˜é‡ï¼Œé¿å…Qtæ’ä»¶é”™è¯¯
if sys.platform.startswith('linux'):
    if not os.environ.get('DISPLAY'):
        os.environ['QT_QPA_PLATFORM'] = 'offscreen'
        # è®¾ç½®matplotlibåç«¯ä¸ºAggï¼ˆå¦‚æœè¿˜æ²¡è®¾ç½®ï¼‰
        import matplotlib
        matplotlib.use('Agg')
import torch

# ==================== è®¾å¤‡è‡ªåŠ¨æ£€æµ‹ ====================
def get_device():
    """è‡ªåŠ¨æ£€æµ‹å¹¶è¿”å›æœ€ä½³å¯ç”¨è®¾å¤‡"""
    if torch.cuda.is_available():
        device = torch.device('cuda')
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"ğŸš€ æ£€æµ‹åˆ°GPU: {gpu_name} ({gpu_memory:.1f}GB)")
        return device
    else:
        print("ğŸ’» ä½¿ç”¨CPUè®­ç»ƒï¼ˆå»ºè®®åœ¨GPUæœåŠ¡å™¨ä¸Šè¿è¡Œä»¥åŠ é€Ÿï¼‰")
        return torch.device('cpu')

# ==================== å¹³å°æ£€æµ‹ ====================
IS_WINDOWS = sys.platform == 'win32'
IS_LINUX = sys.platform.startswith('linux')
print(f"ğŸ“ è¿è¡Œå¹³å°: {'Windows' if IS_WINDOWS else 'Linux' if IS_LINUX else sys.platform}")

# ==================== è·¯å¾„é…ç½® ====================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATASET_DIR = os.path.join(BASE_DIR, 'dataset')
MODELS_DIR = os.path.join(BASE_DIR, 'models')
RESULTS_DIR = os.path.join(BASE_DIR, 'results')
LOGS_DIR = os.path.join(BASE_DIR, 'logs')

# åˆ›å»ºå¿…è¦çš„ç›®å½•
for dir_path in [MODELS_DIR, RESULTS_DIR, LOGS_DIR]:
    os.makedirs(dir_path, exist_ok=True)

# ==================== æ•°æ®é…ç½® ====================
# æ•°æ®é›†è·¯å¾„
DATA_PATHS = {
    '10m': os.path.join(DATASET_DIR, 'WindSpeed_10m', 'data'),
    '50m': os.path.join(DATASET_DIR, 'WindSpeed_50m', 'data'),
    '100m': os.path.join(DATASET_DIR, 'WindSpeed_100m', 'data'),
}

# ç‰¹å¾åˆ—åï¼ˆåŸå§‹æ•°æ®é›†ä¸­çš„åˆ—åï¼‰
FEATURE_COLS = [
    'DirectionAvg',      # é£å‘
    'TemperatureAvg',    # æ¸©åº¦
    'PressureAvg',       # æ°”å‹
    'HumidtyAvg',        # æ¹¿åº¦
]

# ç›®æ ‡åˆ—ï¼ˆæˆ‘ä»¬è¦é¢„æµ‹çš„ï¼‰
TARGET_COL = 'SpeedAvg'  # é£é€Ÿ

# æ—¶é—´æˆ³åˆ—
TIMESTAMP_COL = 'Date & Time Stamp'

# é«˜åº¦åˆ—
HEIGHT_COL = 'height'

# ==================== åºåˆ—é…ç½® ====================
# å•æ­¥é¢„æµ‹é…ç½®ï¼š8å°æ—¶å†å²æ•°æ® â†’ é¢„æµ‹1å°æ—¶
SINGLE_STEP_INPUT_LEN = 8   # è¾“å…¥åºåˆ—é•¿åº¦ï¼ˆ8å°æ—¶ï¼‰
SINGLE_STEP_OUTPUT_LEN = 1  # è¾“å‡ºåºåˆ—é•¿åº¦ï¼ˆ1å°æ—¶ï¼‰

# å¤šæ­¥é¢„æµ‹é…ç½®ï¼š8å°æ—¶å†å²æ•°æ® â†’ é¢„æµ‹16å°æ—¶
MULTI_STEP_INPUT_LEN = 8    # è¾“å…¥åºåˆ—é•¿åº¦ï¼ˆ8å°æ—¶ï¼‰
MULTI_STEP_OUTPUT_LEN = 16  # è¾“å‡ºåºåˆ—é•¿åº¦ï¼ˆ16å°æ—¶ï¼‰

# ==================== æ•°æ®é›†åˆ’åˆ†é…ç½® ====================
TRAIN_RATIO = 0.7
VAL_RATIO = 0.2
TEST_RATIO = 0.1

# ==================== è®­ç»ƒé…ç½® ====================
DEVICE = get_device()

# æ ¹æ®è®¾å¤‡è‡ªåŠ¨è°ƒæ•´batch_size
# GPUå¯ä»¥ä½¿ç”¨æ›´å¤§çš„batch_sizeåŠ é€Ÿè®­ç»ƒ
if torch.cuda.is_available():
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    gpu_name = torch.cuda.get_device_name(0).lower()
    
    # A100/H100ç­‰é«˜ç«¯GPUçš„ä¼˜åŒ–é…ç½®
    if 'a100' in gpu_name or 'h100' in gpu_name or gpu_memory >= 35:
        BATCH_SIZE = 512       # A100 40Gå¯ä»¥ä½¿ç”¨å¾ˆå¤§çš„batch
        USE_AMP = True         # å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
        NUM_WORKERS = 8        # æ›´å¤šæ•°æ®åŠ è½½çº¿ç¨‹
    elif gpu_memory >= 20:     # RTX 3090/4090ç­‰
        BATCH_SIZE = 256
        USE_AMP = True
        NUM_WORKERS = 4
    elif gpu_memory >= 10:     # RTX 3060 12GBç­‰
        BATCH_SIZE = 128
        USE_AMP = True
        NUM_WORKERS = 4
    elif gpu_memory >= 6:
        BATCH_SIZE = 64
        USE_AMP = False
        NUM_WORKERS = 2
    else:
        BATCH_SIZE = 32
        USE_AMP = False
        NUM_WORKERS = 2
else:
    BATCH_SIZE = 64  # CPUé»˜è®¤
    USE_AMP = False
    NUM_WORKERS = 0

LEARNING_RATE = 0.001
NUM_EPOCHS = 100
EARLY_STOPPING_PATIENCE = 15
WEIGHT_DECAY = 1e-5

# ==================== ä»»åŠ¡ç‰¹å®šçš„è¶…å‚ä¼˜åŒ– ====================
# ä¸åŒä»»åŠ¡éœ€è¦ä¸åŒçš„å­¦ä¹ ç‡å’Œæ—©åœè€å¿ƒå€¼
TASK_SPECIFIC_HYPERPARAMS = {
    'singlestep': {
        'lr': 0.001,          # çŸ­æœŸé¢„æµ‹ï¼šæ­£å¸¸å­¦ä¹ ç‡
        'patience': 20,       # æ ‡å‡†æ—©åœ
        'min_epochs': 50,     # è‡³å°‘è®­ç»ƒ50ä¸ªepoch
    },
    'multistep_16h': {
        'lr': 0.0001,         # é•¿æœŸé¢„æµ‹ï¼šæ›´ä½å­¦ä¹ ç‡é¿å…å¿«é€Ÿæ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜
        'patience': 40,       # æ›´å®½æ¾æ—©åœï¼Œå…è®¸æ›´å……åˆ†æ¢ç´¢
        'min_epochs': 100,    # è‡³å°‘è®­ç»ƒ100ä¸ªepoch
    }
}

# æ ¹æ®batch_sizeè‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡ï¼ˆçº¿æ€§ç¼©æ”¾ï¼‰
def get_adjusted_lr(base_lr, batch_size):
    """
    æ ¹æ®batch_sizeè°ƒæ•´å­¦ä¹ ç‡
    çº¿æ€§ç¼©æ”¾æ³•åˆ™: lr = base_lr * (batch_size / 128)
    """
    reference_batch = 128
    return base_lr * (batch_size / reference_batch)

# æ³¨æ„ï¼šå®é™…batch_sizeå¯èƒ½è¢«å‘½ä»¤è¡Œå‚æ•°è¦†ç›–ï¼Œè¿™é‡Œåªæ˜¯é»˜è®¤å€¼
# print(f"âš™ï¸  é»˜è®¤ Batch Size: {BATCH_SIZE}")  # ç§»åˆ°main.pyä¸­æ‰“å°å®é™…ä½¿ç”¨çš„å€¼

# ==================== æ¨¡å‹é…ç½® ====================
# Linearæ¨¡å‹é…ç½®
LINEAR_CONFIG = {
    'hidden_sizes': [128, 64, 32],
    'dropout': 0.2,
}

# LSTMæ¨¡å‹é…ç½®
LSTM_CONFIG = {
    'hidden_size': 256,      # éšè—å±‚å¤§å°
    'num_layers': 3,         # å±‚æ•°
    'dropout': 0.3,          # dropoutç‡
    'bidirectional': True,
}

# Transformeræ¨¡å‹é…ç½®
TRANSFORMER_CONFIG = {
    'd_model': 128,            # æ¨¡å‹ç»´åº¦
    'nhead': 8,                # æ³¨æ„åŠ›å¤´æ•°
    'num_encoder_layers': 3,   # ç¼–ç å™¨å±‚æ•°
    'num_decoder_layers': 3,   # è§£ç å™¨å±‚æ•°
    'dim_feedforward': 512,    # å‰é¦ˆå±‚ç»´åº¦
    'dropout': 0.2,            # dropoutç‡
}

# ==================== åˆ›æ–°æ¨¡å‹é…ç½® ====================
# CNN-LSTMæ··åˆæ¨¡å‹é…ç½®
CNN_LSTM_CONFIG = {
    'cnn_channels': [32, 64],      # CNNé€šé“æ•°
    'kernel_size': 3,
    'lstm_hidden_size': 64,        # LSTMéšè—å±‚å¤§å°
    'lstm_num_layers': 2,          # LSTMå±‚æ•°
    'dropout': 0.3,                # dropoutç‡
}

# TCN (Temporal Convolutional Network) é…ç½®
TCN_CONFIG = {
    'num_channels': [32, 64, 64],  # å„å±‚é€šé“æ•°
    'kernel_size': 3,
    'dropout': 0.3,                # dropoutç‡
}

# WaveNetæ¨¡å‹é…ç½®
WAVENET_CONFIG = {
    'num_channels': 64,            # é€šé“æ•°
    'num_blocks': 8,               # æ®‹å·®å—æ•°é‡
    'kernel_size': 2,              # å·ç§¯æ ¸å¤§å°
    'dropout': 0.3,                # dropoutç‡
}

# é›†æˆæ¨¡å‹é…ç½®
ENSEMBLE_CONFIG = {
    'models': ['Linear', 'LSTM', 'Transformer'],
    'weights': 'learned',  # 'equal', 'learned', 'stacking'
}

# LSTNetæ¨¡å‹é…ç½®ï¼ˆè½»é‡çº§ï¼Œé€‚åˆå°æ•°æ®é›†ï¼‰
LSTNET_CONFIG = {
    'cnn_channels': 32,         # CNNé€šé“æ•°
    'cnn_kernel': 3,            # CNNå·ç§¯æ ¸å¤§å°
    'rnn_hidden': 64,           # GRUéšè—å±‚
    'skip_hidden': 32,          # Skip-GRUéšè—å±‚
    'skip': 4,                  # è·³è·ƒæ­¥é•¿ï¼ˆç”¨äºæ•è·å‘¨æœŸæ€§ï¼‰
    'highway_window': 4,        # è‡ªå›å½’çª—å£
    'dropout': 0.2,             # dropoutç‡
}

# ==================== éšæœºç§å­ ====================
RANDOM_SEED = 42

def set_seed(seed=RANDOM_SEED):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯å¤ç°æ€§"""
    import random
    import numpy as np
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
