"""
é…ç½®æ–‡ä»¶ï¼šå®šä¹‰æ‰€æœ‰è¶…å‚æ•°å’Œè·¯å¾„é…ç½®
æ”¯æŒè‡ªåŠ¨æ£€æµ‹GPUï¼Œé€‚é…æœ¬åœ°Windowså’Œè¿œç¨‹LinuxæœåŠ¡å™¨è®­ç»ƒ
"""
import os
import sys
# ==================== è§£å†³æœåŠ¡å™¨æ— å›¾å½¢ç•Œé¢é—®é¢˜ï¼ˆå¿…é¡»æœ€å…ˆæ‰§è¡Œï¼‰====================
# åœ¨æ— å¤´LinuxæœåŠ¡å™¨ä¸Šè®¾ç½®ç¯å¢ƒå˜é‡ï¼Œé¿å…Qtæ’ä»¶é”™è¯¯
if sys.platform.startswith('linux'):
    if not os.environ.get('DISPLAY'):
        os.environ['QT_QPA_PLATFORM'] = 'offscreen'
        # è®¾ç½®matplotlibåç«¯ä¸ºAggï¼ˆå¦‚æœè¿˜æ²¡è®¾ç½®ï¼‰
        import matplotlib
        matplotlib.use('Agg')
import torch

# ==================== è®¾å¤‡è‡ªåŠ¨æ£€æµ‹ ====================
def get_device():
    """è‡ªåŠ¨æ£€æµ‹å¹¶è¿”å›æœ€ä½³å¯ç”¨è®¾å¤‡"""
    if torch.cuda.is_available():
        device = torch.device('cuda')
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"ğŸš€ æ£€æµ‹åˆ°GPU: {gpu_name} ({gpu_memory:.1f}GB)")
        return device
    else:
        print("ğŸ’» ä½¿ç”¨CPUè®­ç»ƒï¼ˆå»ºè®®åœ¨GPUæœåŠ¡å™¨ä¸Šè¿è¡Œä»¥åŠ é€Ÿï¼‰")
        return torch.device('cpu')

# ==================== å¹³å°æ£€æµ‹ ====================
IS_WINDOWS = sys.platform == 'win32'
IS_LINUX = sys.platform.startswith('linux')
print(f"ğŸ“ è¿è¡Œå¹³å°: {'Windows' if IS_WINDOWS else 'Linux' if IS_LINUX else sys.platform}")

# ==================== è·¯å¾„é…ç½® ====================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATASET_DIR = os.path.join(BASE_DIR, 'dataset')
MODELS_DIR = os.path.join(BASE_DIR, 'models')
RESULTS_DIR = os.path.join(BASE_DIR, 'results')
LOGS_DIR = os.path.join(BASE_DIR, 'logs')

# åˆ›å»ºå¿…è¦çš„ç›®å½•
for dir_path in [MODELS_DIR, RESULTS_DIR, LOGS_DIR]:
    os.makedirs(dir_path, exist_ok=True)

# ==================== æ•°æ®é…ç½® ====================
# æ•°æ®é›†è·¯å¾„
DATA_PATHS = {
    '10m': os.path.join(DATASET_DIR, 'WindSpeed_10m', 'data'),
    '50m': os.path.join(DATASET_DIR, 'WindSpeed_50m', 'data'),
    '100m': os.path.join(DATASET_DIR, 'WindSpeed_100m', 'data'),
}

# ç‰¹å¾åˆ—åï¼ˆåŸå§‹æ•°æ®é›†ä¸­çš„åˆ—åï¼‰
FEATURE_COLS = [
    'DirectionAvg',      # é£å‘
    'TemperatureAvg',    # æ¸©åº¦
    'PressureAvg',       # æ°”å‹
    'HumidtyAvg',        # æ¹¿åº¦
]

# ç›®æ ‡åˆ—ï¼ˆæˆ‘ä»¬è¦é¢„æµ‹çš„ï¼‰
TARGET_COL = 'SpeedAvg'  # é£é€Ÿ

# æ—¶é—´æˆ³åˆ—
TIMESTAMP_COL = 'Date & Time Stamp'

# é«˜åº¦åˆ—
HEIGHT_COL = 'height'

# ==================== åºåˆ—é…ç½® ====================
# å•æ­¥é¢„æµ‹é…ç½®
SINGLE_STEP_INPUT_LEN = 8   # è¾“å…¥åºåˆ—é•¿åº¦ï¼ˆ8å°æ—¶ï¼‰
SINGLE_STEP_OUTPUT_LEN = 1  # è¾“å‡ºåºåˆ—é•¿åº¦ï¼ˆ1å°æ—¶ï¼‰

# å¤šæ­¥é¢„æµ‹é…ç½® - ä»»åŠ¡1ï¼š8å°æ—¶é¢„æµ‹1å°æ—¶
MULTI_STEP_1_INPUT_LEN = 8
MULTI_STEP_1_OUTPUT_LEN = 1

# å¤šæ­¥é¢„æµ‹é…ç½® - ä»»åŠ¡2ï¼š8å°æ—¶é¢„æµ‹16å°æ—¶ï¼ˆç¬¦åˆä½œä¸šè¦æ±‚ï¼‰
MULTI_STEP_2_INPUT_LEN = 8   # ä½œä¸šè¦æ±‚ï¼š8å°æ—¶
MULTI_STEP_2_OUTPUT_LEN = 16

# ==================== æ•°æ®é›†åˆ’åˆ†é…ç½® ====================
TRAIN_RATIO = 0.7
VAL_RATIO = 0.2
TEST_RATIO = 0.1

# ==================== è®­ç»ƒé…ç½® ====================
DEVICE = get_device()

# æ ¹æ®è®¾å¤‡è‡ªåŠ¨è°ƒæ•´batch_size
# GPUå¯ä»¥ä½¿ç”¨æ›´å¤§çš„batch_sizeåŠ é€Ÿè®­ç»ƒ
if torch.cuda.is_available():
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    if gpu_memory >= 10:  # RTX 3060 æœ‰12GBæ˜¾å­˜
        BATCH_SIZE = 128
    elif gpu_memory >= 6:
        BATCH_SIZE = 64
    else:
        BATCH_SIZE = 32
else:
    BATCH_SIZE = 64  # CPUé»˜è®¤

LEARNING_RATE = 0.001
NUM_EPOCHS = 100
EARLY_STOPPING_PATIENCE = 15
WEIGHT_DECAY = 1e-5

# ==================== ä»»åŠ¡ç‰¹å®šçš„è¶…å‚ä¼˜åŒ– ====================
# ä¸åŒä»»åŠ¡éœ€è¦ä¸åŒçš„å­¦ä¹ ç‡å’Œæ—©åœè€å¿ƒå€¼
TASK_SPECIFIC_HYPERPARAMS = {
    'singlestep': {
        'lr': 0.001,          # çŸ­æœŸé¢„æµ‹ï¼šæ­£å¸¸å­¦ä¹ ç‡
        'patience': 15,       # æ ‡å‡†æ—©åœ
        'min_epochs': 50,     # è‡³å°‘è®­ç»ƒ50ä¸ªepoch
    },
    'multistep_1h': {
        'lr': 0.0008,         # å¤šæ­¥1hï¼šç•¥é™å­¦ä¹ ç‡
        'patience': 18,       # ç•¥å®½æ¾
        'min_epochs': 60,
    },
    'multistep_16h': {
        'lr': 0.0003,         # é•¿æœŸé¢„æµ‹ï¼šæ˜¾è‘—é™ä½å­¦ä¹ ç‡ï¼ˆå…³é”®ï¼ï¼‰
        'patience': 25,       # å®½æ¾æ—©åœï¼Œå…è®¸æ›´å¤šæ¢ç´¢
        'min_epochs': 80,     # è‡³å°‘è®­ç»ƒ80ä¸ªepoch
    }
}

# æ ¹æ®batch_sizeè‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡ï¼ˆçº¿æ€§ç¼©æ”¾ï¼‰
def get_adjusted_lr(base_lr, batch_size):
    """
    æ ¹æ®batch_sizeè°ƒæ•´å­¦ä¹ ç‡
    çº¿æ€§ç¼©æ”¾æ³•åˆ™: lr = base_lr * (batch_size / 128)
    """
    reference_batch = 128
    return base_lr * (batch_size / reference_batch)

print(f"âš™ï¸  Batch Size: {BATCH_SIZE}")

# ==================== æ¨¡å‹é…ç½® ====================
# Linearæ¨¡å‹é…ç½®
LINEAR_CONFIG = {
    'hidden_sizes': [128, 64, 32],
    'dropout': 0.2,
}

# LSTMæ¨¡å‹é…ç½®
LSTM_CONFIG = {
    'hidden_size': 256,      # å¢å¤§éšè—å±‚ï¼ˆåŸ128ï¼‰
    'num_layers': 3,         # å¢åŠ å±‚æ•°ï¼ˆåŸ2ï¼‰
    'dropout': 0.3,          # å¢åŠ dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
    'bidirectional': True,
}

# Transformeræ¨¡å‹é…ç½®
TRANSFORMER_CONFIG = {
    'd_model': 128,            # å¢å¤§æ¨¡å‹ç»´åº¦ï¼ˆåŸ64ï¼‰
    'nhead': 8,                # å¢åŠ æ³¨æ„åŠ›å¤´æ•°ï¼ˆåŸ4ï¼‰
    'num_encoder_layers': 4,   # å¢åŠ å±‚æ•°ï¼ˆåŸ3ï¼‰
    'num_decoder_layers': 4,   # å¢åŠ å±‚æ•°ï¼ˆåŸ3ï¼‰
    'dim_feedforward': 512,    # å¢å¤§å‰é¦ˆå±‚ï¼ˆåŸ256ï¼‰
    'dropout': 0.2,            # å¢åŠ dropout
}

# ==================== åˆ›æ–°æ¨¡å‹é…ç½® ====================
# CNN-LSTMæ··åˆæ¨¡å‹é…ç½®
CNN_LSTM_CONFIG = {
    'cnn_channels': [32, 32],      # å‡å°‘é€šé“æ•°ï¼ˆåŸ[32,64]ï¼‰
    'kernel_size': 3,
    'lstm_hidden_size': 64,        # å‡å°‘éšè—å•å…ƒï¼ˆåŸ64ï¼Œå·²åˆç†ï¼‰
    'lstm_num_layers': 2,          # å‡å°‘å±‚æ•°ï¼ˆåŸ2ï¼Œå·²åˆç†ï¼‰
    'dropout': 0.3,                # å¢åŠ dropouté˜²æ­¢è¿‡æ‹Ÿåˆï¼ˆåŸ0.2ï¼‰
}

# Attention-LSTMæ¨¡å‹é…ç½®
ATTENTION_LSTM_CONFIG = {
    'hidden_size': 96,             # å‡å°‘éšè—å•å…ƒï¼ˆåŸ128ï¼‰
    'num_layers': 2,               # ä¿æŒ2å±‚ï¼ˆè¶³å¤Ÿäº†ï¼‰
    'attention_heads': 4,          # å‡å°‘å¤´æ•°ï¼ˆåŸ4ï¼Œå·²åˆç†ï¼‰
    'dropout': 0.3,                # å¢åŠ dropoutï¼ˆåŸ0.2ï¼‰
}

# TCN (Temporal Convolutional Network) é…ç½®
# ä¼˜åŒ–ç‰ˆæœ¬ï¼šå‡å°‘é€šé“æ•°ä»¥åŠ å¿«è®­ç»ƒé€Ÿåº¦
TCN_CONFIG = {
    'num_channels': [32, 64, 64],  # ä¿æŒé€šé“æ•°ï¼ˆå·²ä¼˜åŒ–ï¼‰
    'kernel_size': 3,
    'dropout': 0.3,                # å¢åŠ dropoutï¼ˆåŸ0.2ï¼‰
}

# WaveNetæ¨¡å‹é…ç½®
WAVENET_CONFIG = {
    'num_channels': 64,            # å‡å°‘é€šé“æ•°ï¼ˆåŸ64ï¼Œå·²åˆç†ï¼‰
    'num_blocks': 8,               # ä¿æŒå—æ•°ï¼ˆ8ä¸ªå·²è¶³å¤Ÿï¼‰
    'kernel_size': 2,              # ä¿æŒå·ç§¯æ ¸ï¼ˆæ ‡å‡†è®¾ç½®ï¼‰
    'dropout': 0.3,                # å¢åŠ dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
}

# é›†æˆæ¨¡å‹é…ç½®
ENSEMBLE_CONFIG = {
    'models': ['Linear', 'LSTM', 'Transformer'],
    'weights': 'learned',  # 'equal', 'learned', 'stacking'
}

# ==================== éšæœºç§å­ ====================
RANDOM_SEED = 42

def set_seed(seed=RANDOM_SEED):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯å¤ç°æ€§"""
    import random
    import numpy as np
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
