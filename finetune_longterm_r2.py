"""
é•¿æœŸé¢„æµ‹æ¨¡å‹ç²¾ç»†å¾®è°ƒè„šæœ¬ - ä½¿ç”¨RÂ²ä½œä¸ºæœ€ä¼˜åˆ¤æ–­æ ‡å‡†
ç›®æ ‡ï¼šé’ˆå¯¹ multistep_16h ä»»åŠ¡è¿›è¡Œç²¾ç»†åŒ–è°ƒä¼˜

å…³é”®åŒºåˆ«ï¼š
1. ä½¿ç”¨ RÂ² è€Œé MSE ä½œä¸ºæ—©åœåˆ¤æ–­æ ‡å‡†ï¼ˆmode='max'ï¼Œè¶Šå¤§è¶Šå¥½ï¼‰
2. æ›´ä½çš„å­¦ä¹ ç‡ï¼Œæ›´é•¿çš„è®­ç»ƒå‘¨æœŸ
3. æ›´å®½æ¾çš„æ—©åœæ¡ä»¶

ä¸ºä»€ä¹ˆ16hä»»åŠ¡ç”¨RÂ²æ›´åˆç†ï¼Ÿ
- 16æ­¥é¢„æµ‹çš„MSEå¤©ç„¶å¾ˆå¤§ï¼ˆ3.5-5.2ï¼‰ï¼Œæ³¢åŠ¨å‰§çƒˆ
- RÂ²è¡¨ç¤ºæ¨¡å‹è§£é‡Šçš„æ–¹å·®æ¯”ä¾‹ï¼ŒèŒƒå›´0-1ï¼Œæ›´ç¨³å®š
- RÂ²èƒ½æ›´å¥½åæ˜ æ¨¡å‹çš„"ç›¸å¯¹é¢„æµ‹èƒ½åŠ›"

ç”¨æ³•ï¼š
    python finetune_longterm_r2.py                     # å¾®è°ƒæ‰€æœ‰æ¨¡å‹
    python finetune_longterm_r2.py --models LSTM       # ä»…å¾®è°ƒæŒ‡å®šæ¨¡å‹
    python finetune_longterm_r2.py --lr 0.0001         # è‡ªå®šä¹‰å­¦ä¹ ç‡
"""

import os
import sys
import argparse
import torch
import torch.nn as nn
import numpy as np
from tqdm import tqdm
from datetime import datetime
from sklearn.metrics import r2_score

# è®¾ç½®æ— å¤´æ¨¡å¼
if sys.platform.startswith('linux') and not os.environ.get('DISPLAY'):
    os.environ['QT_QPA_PLATFORM'] = 'offscreen'
    import matplotlib
    matplotlib.use('Agg')

from config import (
    DEVICE, MODELS_DIR, RESULTS_DIR, set_seed, RANDOM_SEED, WEIGHT_DECAY,
    MULTI_STEP_2_INPUT_LEN, MULTI_STEP_2_OUTPUT_LEN,
)
from data_loader import load_all_data, preprocess_data, create_dataloaders
from models import get_model, count_parameters
from models_innovative import get_innovative_model
from trainer import calculate_metrics, test_model, print_test_results, load_model


# ==================== é•¿æœŸä»»åŠ¡å¾®è°ƒè¶…å‚ ====================
FINETUNE_CONFIG = {
    'lr': 0.0002,              # éå¸¸ä½çš„å­¦ä¹ ç‡
    'epochs': 120,             # å……è¶³çš„è®­ç»ƒè½®æ•°
    'patience': 35,            # éå¸¸å®½æ¾çš„æ—©åœ
    'warmup_epochs': 10,       # å­¦ä¹ ç‡é¢„çƒ­
    'lr_min_factor': 0.01,     # æœ€å°å­¦ä¹ ç‡æ¯”ä¾‹
    'description': 'é•¿æœŸé¢„æµ‹å¾®è°ƒ (24hâ†’16h) - RÂ²ä¼˜åŒ–',
}

# æ‰€æœ‰å¯ç”¨æ¨¡å‹
ALL_MODELS = ['Linear', 'LSTM', 'Transformer', 'CNN_LSTM', 'Attention_LSTM', 'TCN', 'WaveNet']
INNOVATIVE_MODELS = ['CNN_LSTM', 'Attention_LSTM', 'TCN', 'WaveNet']


class EarlyStoppingR2:
    """åŸºäºRÂ²çš„æ—©åœæœºåˆ¶ï¼ˆmode='max'ï¼Œè¶Šå¤§è¶Šå¥½ï¼‰"""
    
    def __init__(self, patience=30, min_delta=0.001):
        """
        åˆå§‹åŒ–æ—©åœ
        
        Args:
            patience: å®¹å¿çš„epochæ•°
            min_delta: æœ€å°æ”¹è¿›é‡
        """
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_r2 = None
        self.early_stop = False
        self.best_model_state = None
    
    def __call__(self, r2, model):
        if self.best_r2 is None:
            self.best_r2 = r2
            self.best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}
        elif r2 > self.best_r2 + self.min_delta:
            # RÂ²æå‡äº†
            self.best_r2 = r2
            self.best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
    
    def load_best_model(self, model):
        """åŠ è½½æœ€ä½³æ¨¡å‹çŠ¶æ€"""
        if self.best_model_state is not None:
            model.load_state_dict(self.best_model_state)


def train_with_r2_criterion(model, train_loader, val_loader, model_name, task_name,
                            num_epochs, learning_rate, patience, device=DEVICE,
                            save_best=True, verbose=True, resume=False):
    """
    ä½¿ç”¨RÂ²ä½œä¸ºæœ€ä¼˜åˆ¤æ–­æ ‡å‡†çš„è®­ç»ƒå‡½æ•°
    
    å…³é”®åŒºåˆ«ï¼š
    - æ—©åœåŸºäºRÂ²ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
    - åŒæ—¶è®°å½•MSEå’ŒRÂ²
    """
    model = model.to(device)
    
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=WEIGHT_DECAY)
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=15, T_mult=2, eta_min=learning_rate * FINETUNE_CONFIG['lr_min_factor']
    )
    
    # ä½¿ç”¨RÂ²æ—©åœï¼ˆmode='max'ï¼‰
    early_stopping = EarlyStoppingR2(patience=patience, min_delta=0.001)
    
    # å°è¯•åŠ è½½å·²æœ‰æ¨¡å‹
    model_path = os.path.join(MODELS_DIR, f"{model_name}_{task_name}.pth")
    start_epoch = 0
    history = {
        'train_loss': [],
        'val_loss': [],
        'val_r2': [],
        'val_metrics': [],
        'best_epoch': 0,
        'best_val_r2': -float('inf'),
        'best_val_loss': float('inf'),
    }
    
    if resume and os.path.exists(model_path):
        try:
            checkpoint = torch.load(model_path, map_location=device, weights_only=False)
            checkpoint_state = checkpoint['model_state_dict']
            current_state = model.state_dict()
            
            # æ£€æŸ¥å…¼å®¹æ€§
            compatible = all(
                key in checkpoint_state and current_state[key].shape == checkpoint_state[key].shape
                for key in current_state.keys()
            )
            
            if compatible:
                model.load_state_dict(checkpoint_state)
                prev_history = checkpoint.get('history', {})
                
                # æ¢å¤å†å²
                history['train_loss'] = prev_history.get('train_loss', [])
                history['val_loss'] = prev_history.get('val_loss', [])
                history['val_r2'] = prev_history.get('val_r2', [])
                history['val_metrics'] = prev_history.get('val_metrics', [])
                
                # å…³é”®ï¼šè·å–å†å²æœ€ä½³RÂ²
                if 'val_metrics' in prev_history and len(prev_history['val_metrics']) > 0:
                    historical_r2_values = [m.get('R2', -1) for m in prev_history['val_metrics']]
                    history['best_val_r2'] = max(historical_r2_values) if historical_r2_values else -float('inf')
                    history['best_val_loss'] = prev_history.get('best_val_loss', float('inf'))
                    history['best_epoch'] = prev_history.get('best_epoch', 0)
                
                start_epoch = len(history['train_loss'])
                if verbose:
                    print(f"âœ… ä»æ£€æŸ¥ç‚¹æ¢å¤: å·²å®Œæˆ {start_epoch} epochs")
                    print(f"   å†å²æœ€ä½³ RÂ²: {history['best_val_r2']:.4f}")
            else:
                if verbose:
                    print("âš ï¸ æ¨¡å‹ç»“æ„ä¸å…¼å®¹ï¼Œä»å¤´è®­ç»ƒ")
        except Exception as e:
            if verbose:
                print(f"âš ï¸ æ— æ³•åŠ è½½æ£€æŸ¥ç‚¹: {e}")
    
    remaining_epochs = max(0, num_epochs - start_epoch)
    
    if verbose:
        print(f"\n{'='*60}")
        print(f"è®­ç»ƒ {model_name} - {task_name} (RÂ²ä¼˜åŒ–)")
        print(f"{'='*60}")
        print(f"è®¾å¤‡: {device}")
        print(f"å­¦ä¹ ç‡: {learning_rate}")
        print(f"å‰©ä½™è½®æ•°: {remaining_epochs}")
        print(f"æ—©åœè€å¿ƒ: {patience} (åŸºäºRÂ²)")
    
    if remaining_epochs == 0:
        if verbose:
            print("âœ… å·²å®ŒæˆæŒ‡å®šè½®æ•°è®­ç»ƒ")
        return history
    
    progress_bar = tqdm(range(remaining_epochs), desc=f"Training {model_name}")
    
    for epoch_idx in progress_bar:
        actual_epoch = start_epoch + epoch_idx
        
        # è®­ç»ƒ
        model.train()
        total_loss = 0
        num_batches = 0
        
        for x, y in train_loader:
            x, y = x.to(device), y.to(device)
            
            optimizer.zero_grad()
            output = model(x)
            loss = criterion(output, y)
            loss.backward()
            
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            total_loss += loss.item()
            num_batches += 1
        
        train_loss = total_loss / num_batches if num_batches > 0 else 0
        
        # éªŒè¯
        model.eval()
        val_loss = 0
        all_preds, all_targets = [], []
        
        with torch.no_grad():
            for x, y in val_loader:
                x, y = x.to(device), y.to(device)
                output = model(x)
                val_loss += criterion(output, y).item()
                all_preds.append(output.cpu().numpy())
                all_targets.append(y.cpu().numpy())
        
        val_loss /= len(val_loader)
        all_preds = np.concatenate(all_preds, axis=0)
        all_targets = np.concatenate(all_targets, axis=0)
        
        # è®¡ç®—RÂ²
        val_metrics = calculate_metrics(all_targets, all_preds)
        val_r2 = val_metrics['R2']
        
        # è®°å½•å†å²
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['val_r2'].append(val_r2)
        history['val_metrics'].append(val_metrics)
        
        # æ›´æ–°æœ€ä½³è®°å½•ï¼ˆåŸºäºRÂ²ï¼‰
        if val_r2 > history['best_val_r2']:
            history['best_val_r2'] = val_r2
            history['best_val_loss'] = val_loss
            history['best_epoch'] = actual_epoch + 1
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(actual_epoch)
        current_lr = optimizer.param_groups[0]['lr']
        
        # æ›´æ–°è¿›åº¦æ¡
        progress_bar.set_postfix({
            'train_loss': f'{train_loss:.4f}',
            'val_loss': f'{val_loss:.4f}',
            'val_RÂ²': f'{val_r2:.4f}',
            'best_RÂ²': f'{history["best_val_r2"]:.4f}',
            'lr': f'{current_lr:.6f}'
        })
        
        # RÂ²æ—©åœæ£€æŸ¥
        early_stopping(val_r2, model)
        if early_stopping.early_stop:
            if verbose:
                print(f"\nâ¹ï¸ æ—©åœè§¦å‘äº epoch {actual_epoch + 1} (RÂ²è¿ç»­{patience}è½®æœªæ”¹è¿›)")
            break
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    early_stopping.load_best_model(model)
    
    # ä¿å­˜æ¨¡å‹
    if save_best:
        torch.save({
            'model_state_dict': model.state_dict(),
            'model_name': model_name,
            'task_name': task_name,
            'history': history,
            'total_epochs': len(history['train_loss']),
            'optimization_target': 'R2',  # æ ‡è®°ä¼˜åŒ–ç›®æ ‡
        }, model_path)
        if verbose:
            print(f"âœ… æ¨¡å‹å·²ä¿å­˜ (æœ€ä½³RÂ²: {history['best_val_r2']:.4f})")
    
    return history


def finetune_model(model_name, task_name, df, config, batch_size=64, verbose=True):
    """å¾®è°ƒå•ä¸ªæ¨¡å‹"""
    input_len = MULTI_STEP_2_INPUT_LEN
    output_len = MULTI_STEP_2_OUTPUT_LEN
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, val_loader, test_loader, scaler_features, scaler_targets, feature_cols, target_cols = \
        create_dataloaders(df, input_len, output_len, batch_size)
    
    num_features = len(feature_cols)
    num_targets = len(target_cols)
    
    # åˆ›å»ºæ¨¡å‹
    is_innovative = model_name in INNOVATIVE_MODELS
    if is_innovative:
        model = get_innovative_model(model_name, input_len, output_len, num_features, num_targets)
    else:
        model = get_model(model_name, input_len, output_len, num_features, num_targets)
    
    if verbose:
        print(f"\n{'='*60}")
        print(f"å¾®è°ƒ {model_name} - {config['description']}")
        print(f"{'='*60}")
        print(f"å‚æ•°é‡: {count_parameters(model):,}")
    
    # ä½¿ç”¨RÂ²ä¼˜åŒ–çš„è®­ç»ƒå‡½æ•°
    history = train_with_r2_criterion(
        model, train_loader, val_loader,
        model_name=model_name,
        task_name=task_name,
        num_epochs=config['epochs'],
        learning_rate=config['lr'],
        patience=config['patience'],
        device=DEVICE,
        save_best=True,
        verbose=verbose,
        resume=True
    )
    
    # æµ‹è¯•
    metrics, metrics_per_target, predictions, targets = test_model(
        model, test_loader, scaler_targets, device=DEVICE
    )
    
    if verbose:
        print_test_results(model_name, task_name, metrics, metrics_per_target, target_cols)
    
    return metrics, history


def main():
    parser = argparse.ArgumentParser(description='é•¿æœŸé¢„æµ‹æ¨¡å‹ç²¾ç»†å¾®è°ƒ (RÂ²ä¼˜åŒ–)')
    parser.add_argument('--models', type=str, nargs='+', default=None,
                        help='æŒ‡å®šè¦å¾®è°ƒçš„æ¨¡å‹ï¼ˆä¸æŒ‡å®šåˆ™å…¨éƒ¨å¾®è°ƒï¼‰')
    parser.add_argument('--lr', type=float, default=None,
                        help='è¦†ç›–é»˜è®¤å­¦ä¹ ç‡')
    parser.add_argument('--epochs', type=int, default=None,
                        help='è¦†ç›–é»˜è®¤è®­ç»ƒè½®æ•°')
    parser.add_argument('--patience', type=int, default=None,
                        help='è¦†ç›–é»˜è®¤æ—©åœè€å¿ƒå€¼')
    parser.add_argument('--batch-size', type=int, default=64,
                        help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--quiet', action='store_true',
                        help='å‡å°‘è¾“å‡º')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    set_seed(RANDOM_SEED)
    
    print("=" * 70)
    print("ğŸ¯ é•¿æœŸé¢„æµ‹æ¨¡å‹ç²¾ç»†å¾®è°ƒ (RÂ²ä¼˜åŒ–)")
    print("=" * 70)
    print(f"è®¾å¤‡: {DEVICE}")
    print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"\nğŸ’¡ å…³é”®åŒºåˆ«: ä½¿ç”¨RÂ²(è€ŒéMSE)ä½œä¸ºæœ€ä¼˜åˆ¤æ–­æ ‡å‡†")
    print(f"   åŸå› : 16hé•¿æœŸé¢„æµ‹çš„MSEæ³¢åŠ¨å¤§ï¼ŒRÂ²æ›´ç¨³å®š")
    
    # åŠ è½½æ•°æ®
    print("\nğŸ“Š åŠ è½½æ•°æ®...")
    raw_df = load_all_data()
    df = preprocess_data(raw_df)
    print(f"æ•°æ®å½¢çŠ¶: {df.shape}")
    
    # ç¡®å®šè¦å¾®è°ƒçš„æ¨¡å‹
    models = args.models if args.models else ALL_MODELS
    
    # éªŒè¯æ¨¡å‹åç§°
    invalid_models = [m for m in models if m not in ALL_MODELS]
    if invalid_models:
        print(f"âš ï¸ æœªçŸ¥æ¨¡å‹: {invalid_models}")
        models = [m for m in models if m in ALL_MODELS]
    
    # é…ç½®
    config = FINETUNE_CONFIG.copy()
    if args.lr is not None:
        config['lr'] = args.lr
    if args.epochs is not None:
        config['epochs'] = args.epochs
    if args.patience is not None:
        config['patience'] = args.patience
    
    print(f"\nğŸ“‹ å¾®è°ƒè®¡åˆ’:")
    print(f"  ä»»åŠ¡: multistep_16h ({MULTI_STEP_2_INPUT_LEN}h â†’ {MULTI_STEP_2_OUTPUT_LEN}h)")
    print(f"  æ¨¡å‹: {models}")
    print(f"  è¶…å‚: lr={config['lr']}, epochs={config['epochs']}, patience={config['patience']}")
    
    # å¾®è°ƒ
    all_results = {}
    
    for model_name in models:
        try:
            metrics, history = finetune_model(
                model_name, 'multistep_16h', df, config,
                batch_size=args.batch_size,
                verbose=not args.quiet
            )
            all_results[model_name] = {
                'metrics': metrics,
                'history': history
            }
        except Exception as e:
            print(f"âŒ å¾®è°ƒ {model_name} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    # ç»“æœæ±‡æ€»
    print("\n" + "=" * 70)
    print("ğŸ“Š å¾®è°ƒç»“æœæ±‡æ€» (æŒ‰RÂ²æ’åº)")
    print("=" * 70)
    print(f"\nã€{config['description']}ã€‘")
    print("-" * 60)
    print(f"{'æ¨¡å‹':<20} {'MSE':>10} {'RMSE':>10} {'MAE':>10} {'RÂ²':>10}")
    print("-" * 60)
    
    # æŒ‰RÂ²æ’åº
    sorted_results = sorted(
        all_results.items(),
        key=lambda x: x[1]['metrics']['R2'],
        reverse=True
    )
    
    for i, (model_name, result) in enumerate(sorted_results):
        m = result['metrics']
        medal = "ğŸ¥‡" if i == 0 else ("ğŸ¥ˆ" if i == 1 else ("ğŸ¥‰" if i == 2 else "  "))
        print(f"{medal}{model_name:<18} {m['MSE']:>10.4f} {m['RMSE']:>10.4f} {m['MAE']:>10.4f} {m['R2']:>10.4f}")
    
    print("\n" + "=" * 70)
    print("âœ… é•¿æœŸé¢„æµ‹å¾®è°ƒå®Œæˆï¼")
    print(f"   æ¨¡å‹å·²ä¿å­˜è‡³: {MODELS_DIR}")
    print("=" * 70)


if __name__ == "__main__":
    main()
